\documentclass[10pt]{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{listings}
\usepackage{enumerate}
\usepackage{float}
\usepackage{rotating}

\lstset{numbers=left, frame=single, basicstyle=\small\ttfamily}

\usepackage[margin=1in]{geometry}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\indep}{\perp\!\!\!\perp}

\title{\textbf{Stat 516}\\Homework 5}
\author{Alden Timme and Marco Ribeiro}
\date{Due Date: Thursday, November 13}

\begin{document}

\maketitle
\subsection*{1}
\begin{enumerate}[(a)]
  \item For the model $Y\vert \theta \sim \mathrm{Poisson}(E \times \theta)$,
    where $E$ is the ``expected'' number of cases, $Y$ is the count of
    disease cases, and $\theta > 0$ is the relative risk, we have the
    likelihood function
    \begin{align*}
      L(\theta) = p_\theta(Y=y) = \frac{(E\theta)^y}{y!}e^{-E\theta}
    \end{align*}
    which yields the log likelihood
    \begin{align*}
      \ell(\theta) = \log L(\theta) = y\log(E\theta) - \log(y!) - E\theta
    \end{align*}
    To find Fisher's (expected) information $I(\theta)$, we use $I(\theta) =
    -\E\left[\ddot\ell(\theta)\right]$,
    \begin{align*}
      \ell(\theta) &= y\log E + y\log \theta - \log(y!) - E\theta\\
      \dot\ell(\theta) &= \frac{y}{\theta} - E\\
      \ddot\ell(\theta) &= -\frac{y}{\theta^2}\\
      I(\theta) &= -\E\left[\ell''(\theta)\right] = -\E\left[-\frac{Y}{\theta^2}\right] = \frac{1}{\theta^2} \E[Y] = \frac{E}{\theta}
    \end{align*}
    Maximizing $\ell(\theta)$ to find the MLE, we have
    \begin{align*}
      \hat\theta &= \frac{y}{E}
    \end{align*}
    The variance of the MLE $\hat\theta$ is then given by
    \begin{align*}
      \Var(\hat\theta) &= I(\theta)^{-1} = \frac{\theta}{E}
    \end{align*}

  \item If we assume a prior of $\theta \sim \mathrm{Gamma}(a,b)$, we have
    \begin{align*}
      p(\theta|y)
      &\propto p(y|\theta) p(\theta)\\
      &\propto \theta^y e^{-E\theta} \theta^{a-1} e^{-b\theta}\\
      &= \theta^{y+a-1}e^{-(E+b)\theta}
    \end{align*}
    so we see $\theta|y \sim \mathrm{Gamma}(y+a, E+b)$.

  \item When we see $y = 4$ cases of leukemia with an expected number
    $E = 0.25$, the MLE is
    \begin{align*}
      \hat\theta &= \frac{y}{E} = 16
    \end{align*}
    with variance
    \begin{align*}
      \Var(\hat\theta) &= \frac{\theta}{E} = 64
    \end{align*}
    Since the MLE is asymptotically normal, we can approximate the $95\%$
    confidence interval with a normal distribution,
    \begin{align*}
      \hat\theta \pm 1.96 \times \sqrt{\Var(\hat\theta)}
      &= 16 \pm 1.96 \times 8
      \approx (\Sexpr{16 - 1.96 * 8}, \Sexpr{16 + 1.96 * 8})
    \end{align*}

  \item To find the $a$ and $b$ which give a gamma prior with $90\%$ interval
    $[0.1,10]$, we use the R function \texttt{optim},
<<gamma-prior-params, cache=T, eval=T, echo=T>>=
priorch <- function(x, q1, q2, p1, p2) {
  (p1 - pgamma(q1, x[1], x[2]))^2 + (p2 - pgamma(q2, x[1], x[2]))^2
}
opt <- optim(par=c(1,1), fn=priorch, q1=0.1, q2=10, p1=0.05, p2=0.95)
a <- opt$par[1]
b <- opt$par[2]
@
    yielding $a=\Sexpr{a}$ and $b=\Sexpr{b}$.\\
    Using this prior and the data from part (c), we arrive at a posterior
    of $\theta | y \sim \mathrm{Gamma}(\Sexpr{4+a}, \Sexpr{0.25+b})$. Sampling
    from this distribution $1000$ times, we get the following histogram,
    \begin{center}
<<gamma-posterior-histogram, cache=T, eval=T, figure=T, echo=F, fig.height=5, fig.width=5>>=
y <- 4
E <- 0.25
atilde <- a + y
btilde <- b + E
set.seed(35)
samples <- rgamma(10000, atilde, btilde)
interval.lower <- qgamma(0.025, atilde, btilde)
interval.upper <- qgamma(0.975, atilde, btilde)
hist(samples, main="10000 Samples from Posterior")
@
    \end{center}
    A $95\%$ credible interval using the $0.025$- and $0.975$-quantiles is
    $(\Sexpr{interval.lower}, \Sexpr{interval.upper})$.

  \item Using the $95\%$ confidence interval from the MLE and its variance, we
    would say there is \textit{not} evidence of excess risk for these data,
    because the value $\theta = 1$ corresponding to ``null'' risk is contained
    in the asymptotic $95\%$ confidence interval. From the Bayesian analysis,
    we would say there \textit{is} evidence of excess risk for these data,
    because the $95\%$ credible interval does not contain $\theta = 1$
    (``null'' risk).
    % TODO(Alden):
    % "discuss" the differences between the likelihood-based and Bayesian analyses

\end{enumerate}

\subsection*{2}
<<snoqualmie, eval=T, echo=F>>=
y <- scan("snoqualmie.txt", quiet=T)
# first year is a leap year, but already accounting for that in starting
# value of "daysum"
nodays <- rep(c(365,365,365,366),9)
z <- matrix(0,nrow=36,ncol=30)
daysum <- 31 + 29 + 31 + 30 + 31
for (i in 1:36) {
  if (i > 1) daysum <- nodays[i-1]
  z[i,] <- y[daysum+(1:30)]
}
n <- matrix(c(0, 0, 0, 0), nrow=2)
for (i in 1:36) {
  for (j in 2:30) {
    if (z[i,j-1] == 0 && z[i,j] == 0) n[1,1] <- n[1,1] + 1
    if (z[i,j-1] == 0 && z[i,j] > 0) n[1,2] <- n[1,2] + 1
    if (z[i,j-1] > 0 && z[i,j] == 0) n[2,1] <- n[2,1] + 1
    if (z[i,j-1] > 0 && z[i,j] > 0) n[2,2] <- n[2,2] + 1
  }
}
nplus <- rowSums(n)
p.hat <- n / cbind(nplus, nplus)
var1 <- p.hat[1,2] * (1-p.hat[1,2]) / nplus[1]
ci1 <- p.hat[1,2] + c(-1,1) * 1.96 * sqrt(var1)
var2 <- p.hat[2,1] * (1-p.hat[2,1]) / nplus[2]
ci2 <- p.hat[2,1] + c(-1,1) * 1.96 * sqrt(var2)
p12.median <- qbeta(0.5, n[1,2] + 1, n[1,1] + 1)
p21.median <- qbeta(0.5, n[2,1] + 1, n[2,2] + 1)
bayes.ci1 <- qbeta(c(0.025, 0.975), n[1,2] + 1, n[1,1] + 1)
bayes.ci2 <- qbeta(c(0.025, 0.975), n[2,1] + 1, n[2,2] + 1)
p.hat.indep <- colSums(n) / sum(n)
lrt.stat <- 2 * sum(n * log(p.hat / cbind(p.hat.indep, p.hat.indep)))
pval <- 1 - pchisq(lrt.stat, df=1)
log.py.h0 <- lchoose(sum(n), nplus[1]) + lgamma(nplus[1] + 1) + lgamma(nplus[2] + 1) - lgamma(sum(n) + 2)
log.py.h1 <- lchoose(nplus[1], n[1,2]) + lchoose(nplus[2], n[2,1]) + lgamma(n[1,1] + 1) + lgamma(n[1,2] + 1) - lgamma(nplus[1] + 2) + lgamma(n[2,1] + 1) + lgamma(n[2,2] + 1) - lgamma(nplus[2] + 2)
@
\begin{enumerate}[(a)]
  \item MLEs of $p_{12}$ $p_{21}$ are given by
    \begin{align*}
      \hat{p}_{12} &= \frac{n_{12}}{n_{1+}}
      = \frac{\Sexpr{n[1,2]}}{\Sexpr{sum(n[1,])}}
      \approx \Sexpr{p.hat[1,2]}\\
      \hat{p}_{21} &= \frac{n_{21}}{n_{2+}}
      = \frac{\Sexpr{n[2,1]}}{\Sexpr{sum(n[2,])}}
      \approx \Sexpr{p.hat[2,1]}
    \end{align*}
    The asymptotic variances of $\hat{p}_{12}$ and $\hat{p}_{21}$ are then
    \begin{align*}
      \widehat{\Var(\hat{p}_{12})}
      &\approx \frac{\hat{p}_{12}(1-\hat{p}_{12})}{n_{1+}}
      \approx \Sexpr{var1}\\
      \widehat{\Var(\hat{p}_{21})}
      &\approx \frac{\hat{p}_{21}(1-\hat{p}_{21})}{n_{2+}}
      \approx \Sexpr{var2}
    \end{align*}
    Using these estimates, we can form $95\%$ asymptotic confidence intervals
    using the asymptotic normality of the MLEs,
    \begin{align*}
      \mathrm{CI}_{12}
      &= \hat{p}_{12} \pm 1.96 \times \sqrt{\widehat{\Var(\hat{p}_{12})}}
      \approx (\Sexpr{ci1[1]}, \Sexpr{ci1[2]})\\
      \mathrm{CI}_{21}
      &= \hat{p}_{21} \pm 1.96 \times \sqrt{\widehat{\Var(\hat{p}_{21})}}
      \approx (\Sexpr{ci2[1]}, \Sexpr{ci2[2]})
    \end{align*}

  \item With independent uniform priors, we have $p_{12} \sim
    \mathrm{Beta}(1,1)$ and $p_{21} \sim \mathrm{Beta}(1,1)$. The posterior
    distributions are then
    \begin{align*}
      p_{12} | n_{1\cdot} &\sim \mathrm{Beta}(n_{12} + 1, n_{11} + 1)\\
      p_{21} | n_{2\cdot} &\sim \mathrm{Beta}(n_{21} + 1, n_{22} + 1)
    \end{align*}
    The posterior medians are then
    \begin{align*}
      \tilde{p}_{12}
      &\approx \frac{n_{12} + 1 - 1/3}{n_{1+} - 2/3}
      \approx \Sexpr{p12.median}\\
      \tilde{p}_{21}
      &\approx \frac{n_{21} + 1 - 1/3}{n_{2+} - 2/3}
      \approx \Sexpr{p21.median}
    \end{align*}
    A $95\%$ credible interval for each parameter can then be obtained from the
    $0.025$- and $0.975$-quantiles of each parameter's respective posterior,
    \begin{align*}
      \mathrm{CI}_{12}
      &\approx (\Sexpr{bayes.ci1[1]}, \Sexpr{bayes.ci1[2]})\\
      \mathrm{CI}_{21}
      &\approx (\Sexpr{bayes.ci2[1]}, \Sexpr{bayes.ci2[2]})
    \end{align*}

  \item Using a likelihood ratio test, we want to test the null hypothesis
    $H_0$ that the weather on each day is independent versus $H_1$ that the
    weather on one day depends on the weather the previous day. Under $H_0$,
    $\hat{p}_{j} = n_j/n$, whereas under $H_1$, $\hat{p}_{ij} = n_{ij}/n_{i+}$.
    Using the likelihood ratio test statistic, we have
    \begin{align*}
      T
      &= 2(\hat\ell_1 - \hat\ell_0)\\
      &= 2\sum_{i=1}^2\sum_{j=1}^2 n_{ij} \log\left(\frac{\hat{p}_{ij}}{\hat{p}_j}\right)\\
      &= 2\sum_{i=1}^2\sum_{j=1}^2 n_{ij} \log\left(\frac{n_{ij}/n_{i+}}{n_j/n}\right)\\
      &\approx \Sexpr{lrt.stat}
    \end{align*}
    Under the null, $T \sim \chi^2_1$, and we have $P(T > \Sexpr{lrt.stat})
    \approx \Sexpr{pval}$, so the likelihood ratio test would reject the null
    hypothesis that the weather on each day is independent.\\
    Under the Bayesian paradigm, we compare the independence assumption with
    prior $p_1 \sim \mathrm{Beta}(1,1)$ versus the Markov assumption with
    priors $p_{12} \sim \mathrm{Beta}(1,1)$ and $p_{21} \sim
    \mathrm{Beta}(1,1)$. Under the independence assumption, the posterior
    for $p_1$ is
    \begin{align*}
      p_1 | n \sim \mathrm{Beta}(n_1 + 1, n_2 + 1)
    \end{align*}
    while under the Markov assumption we have, as before,
    \begin{align*}
      p_{12} | n_{1\cdot} &\sim \mathrm{Beta}(n_{12} + 1, n_{11} + 1)\\
      p_{21} | n_{2\cdot} &\sim \mathrm{Beta}(n_{21} + 1, n_{22} + 1)
    \end{align*}
    To find the Bayes factor, we must compute $\Pr(y|H_0)$ and $\Pr(y|H_1)$.
    For $H_0$, we have
    \begin{align*}
      \Pr(y|H_0)
      &= {n \choose n_1} \frac{\Gamma(2)}{\Gamma(1)\Gamma(1)}
      \frac{\Gamma(n_1 + 1)\Gamma(n_2 + 1)}{\Gamma(n+2)}\\
      &= {n \choose n_1} \frac{\Gamma(n_1 + 1)\Gamma(n_2 + 1)}{\Gamma(n+2)}\\
      &\approx \Sexpr{exp(log.py.h0)}
    \end{align*}
    while under $H_1$, we have
    \begin{align*}
      \Pr(y|H_1)
      &= \int_0^1 \int_0^1 p_{12}^{n_{12}} (1-p_{12})^{n_{11}} p_{21}^{n_{21}} (1-p_{21})^{n_{22}}
      \left(\frac{\Gamma(2)}{\Gamma(1)\Gamma(1)}\right)^2 dp_{12} dp_{21}
      {n_{1+} \choose n_{12}}{n_{2+} \choose n_{21}}\\
      &= {n_{1+} \choose n_{12}}{n_{2+} \choose n_{21}}
      \int_0^1 \int_0^1 p_{12}^{n_{12}} (1-p_{12})^{n_{11}} p_{21}^{n_{21}} (1-p_{21})^{n_{22}} dp_{12} dp_{21}\\
      &= {n_{1+} \choose n_{12}}{n_{2+} \choose n_{21}}
      \int_0^1 p_{12}^{n_{12}} (1-p_{12})^{n_{11}} dp_{12} \times
      \int_0^1 p_{21}^{n_{21}} (1-p_{21})^{n_{22}} dp_{21}\\
      &= {n_{1+} \choose n_{12}}{n_{2+} \choose n_{21}}
      \frac{\Gamma(n_{12}+1)\Gamma(n_{11}+1)}{\Gamma(n_{1+} + 2)}
      \cdot \frac{\Gamma(n_{21}+1)\Gamma(n_{22}+1)}{\Gamma(n_{2+} + 2)}\\
      &\approx \Sexpr{exp(log.py.h1)}
    \end{align*}
    The Bayes factor is then
    \begin{align*}
      \mathrm{BF} = \frac{\Pr(y|H_0)}{\Pr(y|H_1)}
      \approx \Sexpr{exp(log.py.h0 - log.py.h1)}
    \end{align*}
    Using the Kass and Raftery suggestions for intervals of Bayes Factors, we
    note that $1/\mathrm{BF} \approx \Sexpr{exp(log.py.h1 - log.py.h0)} < 1$,
    so we would say there is \textit{not} sufficient evidence against the null
    hypothesis of independence.

\end{enumerate}

\end{document}
