\documentclass[10pt]{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{listings}
\usepackage{enumerate}
\usepackage{float}
\usepackage{rotating}

\lstset{numbers=left, frame=single, basicstyle=\small\ttfamily}

\usepackage[margin=1in]{geometry}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\indep}{\perp\!\!\!\perp}
\newcommand{\Indicator}[1]{\mathbbm{1}_{\{#1\}}}
\newcommand{\Info}{\mathcal{I}}

\title{\textbf{Stat 516}\\Homework 5}
\author{Alden Timme and Marco Ribeiro}
\date{Due Date: Thursday, November 13}

\begin{document}
<<libaries, eval=T, echo=F>>=
library(ggplot2)
@

\maketitle
\subsection*{1}
\begin{enumerate}[(a)]
  \item For the model $Y\vert \theta \sim \mathrm{Poisson}(E \times \theta)$,
    where $E$ is the ``expected'' number of cases, $Y$ is the count of
    disease cases, and $\theta > 0$ is the relative risk, we have the
    likelihood function
    \begin{align*}
      L(\theta) = p_\theta(Y=y) = \frac{(E\theta)^y}{y!}e^{-E\theta}
    \end{align*}
    which yields the log likelihood
    \begin{align*}
      \ell(\theta) = \log L(\theta) = y\log(E\theta) - \log(y!) - E\theta
    \end{align*}

    To find Fisher's (expected) information $I(\theta)$, we use $I(\theta) =
    -\E\left[\ddot\ell(\theta)\right]$,
    \begin{align*}
      \ell(\theta) &= y\log E + y\log \theta - \log(y!) - E\theta\\
      \dot\ell(\theta) &= S(\theta) =  \frac{y}{\theta} - E\\
      \ddot\ell(\theta) &= -\frac{y}{\theta^2}\\
      I(\theta) &= -\E\left[\ell''(\theta)\right] = -\E\left[-\frac{Y}{\theta^2}\right] = \frac{1}{\theta^2} \E[Y] = \frac{E}{\theta}
    \end{align*}
    Maximizing $\ell(\theta)$ to find the MLE, we have
    \begin{align*}
      \hat\theta &= \frac{y}{E}
    \end{align*}
    The variance of the MLE $\hat\theta$ is then given by
    \begin{align*}
      \Var(\hat\theta) &= I(\theta)^{-1} = \frac{\theta}{E}
    \end{align*}

  \item If we assume a prior of $\theta \sim \mathrm{Gamma}(a,b)$, we have
    \begin{align*}
      p(\theta|y)
      &\propto p(y|\theta) p(\theta)\\
      &\propto \theta^y e^{-E\theta} \theta^{a-1} e^{-b\theta}\\
      &= \theta^{y+a-1}e^{-(E+b)\theta}
    \end{align*}
    so we see $\theta|y \sim \mathrm{Gamma}(y+a, E+b)$.

  \item When we see $y = 4$ cases of leukemia with an expected number
    $E = 0.25$, the MLE is
    \begin{align*}
      \hat\theta &= \frac{y}{E} = 16
    \end{align*}
    with variance
    \begin{align*}
      \Var(\hat\theta) &= \frac{\theta}{E} = 64
    \end{align*}
    Since the MLE is asymptotically normal, we can approximate the $95\%$
    confidence interval with a normal distribution,
    \begin{align*}
      \hat\theta \pm 1.96 \times \sqrt{\Var(\hat\theta)}
      &= 16 \pm 1.96 \times 8
      \approx (\Sexpr{16 - 1.96 * 8}, \Sexpr{16 + 1.96 * 8})
    \end{align*}

  \item To find the $a$ and $b$ which give a gamma prior with $90\%$ interval
    $[0.1,10]$, we use the R function \texttt{optim},
<<gamma-prior-params, cache=T, eval=T, echo=T>>=
priorch <- function(x, q1, q2, p1, p2) {
  (p1 - pgamma(q1, x[1], x[2]))^2 + (p2 - pgamma(q2, x[1], x[2]))^2
}
opt <- optim(par=c(1,1), fn=priorch, q1=0.1, q2=10, p1=0.05, p2=0.95)
a <- opt$par[1]
b <- opt$par[2]
@
    yielding $a=\Sexpr{a}$ and $b=\Sexpr{b}$.\\
    Using this prior and the data from part (c), we arrive at a posterior
    of $\theta | y \sim \mathrm{Gamma}(\Sexpr{4+a}, \Sexpr{0.25+b})$. Sampling
    from this distribution $1000$ times, we get the following histogram,
    \begin{center}
<<gamma-posterior-histogram, cache=T, eval=T, figure=T, echo=F, fig.height=5, fig.width=5>>=
y <- 4
E <- 0.25
atilde <- a + y
btilde <- b + E
set.seed(35)
samples_y <- rgamma(10000, atilde, btilde)
interval.lower <- qgamma(0.025, atilde, btilde)
interval.upper <- qgamma(0.975, atilde, btilde)
hist(samples_y, main="10000 Samples from Posterior")
@
    \end{center}
    A $95\%$ credible interval using the $0.025$- and $0.975$-quantiles is
    $(\Sexpr{interval.lower}, \Sexpr{interval.upper})$.

  \item Using the $95\%$ confidence interval from the MLE and its variance, we
    would say there is \textit{not} evidence of excess risk for these data,
    because the value $\theta = 1$ corresponding to ``null'' risk is contained
    in the asymptotic $95\%$ confidence interval. From the Bayesian analysis,
    we would say there \textit{is} evidence of excess risk for these data,
    because the $95\%$ credible interval does not contain $\theta = 1$
    (``null'' risk).
    
    Since we only have one observation, the confidence interval
    from the MLE is very wide. The prior in the Bayesian analysis acts as if we
    had ``previous observations'', thus tightening the confidence interval.

\end{enumerate}

\subsection*{2}
<<snoqualmie, eval=T, echo=F>>=
y <- scan("snoqualmie.txt", quiet=T)
# first year is a leap year, but already accounting for that in starting
# value of "daysum"
nodays <- rep(c(365,365,365,366),9)
z <- matrix(0,nrow=36,ncol=30)
daysum <- 31 + 29 + 31 + 30 + 31
for (i in 1:36) {
  if (i > 1) daysum <- daysum +  nodays[i - 1]
  z[i,] <- y[daysum+(1:30)]
}
n <- matrix(c(0, 0, 0, 0), nrow=2)
for (i in 1:36) {
  for (j in 2:30) {
    if (z[i,j-1] == 0 && z[i,j] == 0) n[1,1] <- n[1,1] + 1
    if (z[i,j-1] == 0 && z[i,j] > 0) n[1,2] <- n[1,2] + 1
    if (z[i,j-1] > 0 && z[i,j] == 0) n[2,1] <- n[2,1] + 1
    if (z[i,j-1] > 0 && z[i,j] > 0) n[2,2] <- n[2,2] + 1
  }
}
nsum = sum(n)
nplus <- rowSums(n)
n1  = colSums(n)[1]
n2  = colSums(n)[2]
p.hat <- n / cbind(nplus, nplus)
var1 <- p.hat[1,2] * (1-p.hat[1,2]) / nplus[1]
ci1 <- p.hat[1,2] + c(-1,1) * 1.96 * sqrt(var1)
var2 <- p.hat[2,1] * (1-p.hat[2,1]) / nplus[2]
ci2 <- p.hat[2,1] + c(-1,1) * 1.96 * sqrt(var2)
p12.median <- qbeta(0.5, n[1,2] + 1, n[1,1] + 1)
p21.median <- qbeta(0.5, n[2,1] + 1, n[2,2] + 1)
bayes.ci1 <- qbeta(c(0.025, 0.975), n[1,2] + 1, n[1,1] + 1)
bayes.ci2 <- qbeta(c(0.025, 0.975), n[2,1] + 1, n[2,2] + 1)
p.hat.indep <- c(n1, n2) / nsum
lrt.stat <- 2 * sum(n * log(p.hat / rbind(p.hat.indep, p.hat.indep)))
pval <- 1 - pchisq(lrt.stat, df=1)
log.py.h0 <- lchoose(nplus[1], n[1,2]) + lchoose(nplus[2], n[2,1]) + lgamma(n1 + 1) + lgamma(n2 + 1) - lgamma(nsum + 2)
log.py.h1 <- lchoose(nplus[1], n[1,2]) + lchoose(nplus[2], n[2,1]) + lgamma(n[1,1] + 1) + lgamma(n[1,2] + 1) - lgamma(nplus[1] + 2) + lgamma(n[2,1] + 1) + lgamma(n[2,2] + 1) - lgamma(nplus[2] + 2)
@
\begin{enumerate}[(a)]
  \item The transition frequencies for June across $36$ years are given by: 
  \[
    \left[ \begin{array}{cc}
      \Sexpr{n[1,1]} & \Sexpr{n[1,2]} \\ 
      \Sexpr{n[2,1]} & \Sexpr{n[2,2]} \end{array}
    \right]
  \]
  MLEs of $p_{12}$ $p_{21}$ are given by
    \begin{align*}
      \hat{p}_{12} &= \frac{n_{12}}{n_{1+}}
      = \frac{\Sexpr{n[1,2]}}{\Sexpr{sum(n[1,])}}
      \approx \Sexpr{p.hat[1,2]}\\
      \hat{p}_{21} &= \frac{n_{21}}{n_{2+}}
      = \frac{\Sexpr{n[2,1]}}{\Sexpr{sum(n[2,])}}
      \approx \Sexpr{p.hat[2,1]}
    \end{align*}
    The asymptotic variances of $\hat{p}_{12}$ and $\hat{p}_{21}$ are then
    \begin{align*}
      \widehat{\Var(\hat{p}_{12})}
      &\approx \frac{\hat{p}_{12}(1-\hat{p}_{12})}{n_{1+}}
      \approx \Sexpr{var1}\\
      \widehat{\Var(\hat{p}_{21})}
      &\approx \frac{\hat{p}_{21}(1-\hat{p}_{21})}{n_{2+}}
      \approx \Sexpr{var2}
    \end{align*}
    Using these estimates, we can form $95\%$ asymptotic confidence intervals
    using the asymptotic normality of the MLEs,
    \begin{align*}
      \mathrm{CI}_{12}
      &= \hat{p}_{12} \pm 1.96 \times \sqrt{\widehat{\Var(\hat{p}_{12})}}
      \approx (\Sexpr{ci1[1]}, \Sexpr{ci1[2]})\\
      \mathrm{CI}_{21}
      &= \hat{p}_{21} \pm 1.96 \times \sqrt{\widehat{\Var(\hat{p}_{21})}}
      \approx (\Sexpr{ci2[1]}, \Sexpr{ci2[2]})
    \end{align*}

  \item With independent uniform priors, we have $p_{12} \sim
    \mathrm{Beta}(1,1)$ and $p_{21} \sim \mathrm{Beta}(1,1)$. The posterior
    distributions are then
    \begin{align*}
      p_{12} | n_{1\cdot} &\sim \mathrm{Beta}(n_{12} + 1, n_{11} + 1)\\
      p_{21} | n_{2\cdot} &\sim \mathrm{Beta}(n_{21} + 1, n_{22} + 1)
    \end{align*}
    The posterior medians are then
    \begin{align*}
      \tilde{p}_{12}
      &\approx \frac{n_{12} + 1 - 1/3}{n_{1+} - 2/3}
      \approx \Sexpr{p12.median}\\
      \tilde{p}_{21}
      &\approx \frac{n_{21} + 1 - 1/3}{n_{2+} - 2/3}
      \approx \Sexpr{p21.median}
    \end{align*}
    A $95\%$ credible interval for each parameter can then be obtained from the
    $0.025$- and $0.975$-quantiles of each parameter's respective posterior,
    \begin{align*}
      \mathrm{CI}_{12}
      &\approx (\Sexpr{bayes.ci1[1]}, \Sexpr{bayes.ci1[2]})\\
      \mathrm{CI}_{21}
      &\approx (\Sexpr{bayes.ci2[1]}, \Sexpr{bayes.ci2[2]})
    \end{align*}

  \item Using a likelihood ratio test, we want to test the null hypothesis
    $H_0$ that the weather on each day is independent versus $H_1$ that the
    weather on one day depends on the weather the previous day. Under $H_0$,
    $\hat{p}_{j} = n_j/n$, whereas under $H_1$, $\hat{p}_{ij} = n_{ij}/n_{i+}$.
    Using the likelihood ratio test statistic, we have
    \begin{align*}
      T
      &= 2(\hat\ell_1 - \hat\ell_0)\\
      &= 2\sum_{i=1}^2\sum_{j=1}^2 n_{ij} \log\left(\frac{\hat{p}_{ij}}{\hat{p}_j}\right)\\
      &= 2\sum_{i=1}^2\sum_{j=1}^2 n_{ij} \log\left(\frac{n_{ij}/n_{i+}}{n_j/n}\right)\\
      &\approx \Sexpr{lrt.stat}
    \end{align*}
    Under the null, $T \sim \chi^2_1$, and we have $\Pr(T > \Sexpr{lrt.stat})
    \approx \Sexpr{pval}$, so the likelihood ratio test would reject the null
    hypothesis that the weather on each day is independent.\\
    Under the Bayesian paradigm, we compare the independence assumption with
    prior $p_1 \sim \mathrm{Beta}(1,1)$ versus the Markov assumption with
    priors $p_{12} \sim \mathrm{Beta}(1,1)$ and $p_{21} \sim
    \mathrm{Beta}(1,1)$. Under the independence assumption, the posterior
    for $p_1$ is
    \begin{align*}
      p_1 | n \sim \mathrm{Beta}(n_1 + 1, n_2 + 1)
    \end{align*}
    while under the Markov assumption we have, as before,
    \begin{align*}
      p_{12} | n_{1\cdot} &\sim \mathrm{Beta}(n_{12} + 1, n_{11} + 1)\\
      p_{21} | n_{2\cdot} &\sim \mathrm{Beta}(n_{21} + 1, n_{22} + 1)
    \end{align*}
    To find the Bayes factor, we must compute $\Pr(\mathbf{n}|H_0)$ and
    $\Pr(\mathbf{n}|H_1)$. For $H_0$, we have
    \begin{align*}
      \Pr(\mathbf{n}|H_0)
      &= {n_{1+} \choose n_{12}} {n_{2+} \choose n_{21}} \frac{\Gamma(2)}{\Gamma(1)\Gamma(1)}
      \frac{\Gamma(n_1 + 1)\Gamma(n_2 + 1)}{\Gamma(n+2)}\\
      &= {n_{1+} \choose n_{12}} {n_{2+} \choose n_{21}} \frac{\Gamma(n_1 + 1)\Gamma(n_2 + 1)}{\Gamma(n+2)}\\
      &\approx \Sexpr{exp(log.py.h0)}
    \end{align*}
    while under $H_1$, we have
    \begin{align*}
      \Pr(\mathbf{n}|H_1)
      &= \int_0^1 \int_0^1 p_{12}^{n_{12}} (1-p_{12})^{n_{11}} p_{21}^{n_{21}} (1-p_{21})^{n_{22}}
      \left(\frac{\Gamma(2)}{\Gamma(1)\Gamma(1)}\right)^2 dp_{12} dp_{21}
      {n_{1+} \choose n_{12}}{n_{2+} \choose n_{21}}\\
      &= {n_{1+} \choose n_{12}}{n_{2+} \choose n_{21}}
      \int_0^1 \int_0^1 p_{12}^{n_{12}} (1-p_{12})^{n_{11}} p_{21}^{n_{21}} (1-p_{21})^{n_{22}} dp_{12} dp_{21}\\
      &= {n_{1+} \choose n_{12}}{n_{2+} \choose n_{21}}
      \int_0^1 p_{12}^{n_{12}} (1-p_{12})^{n_{11}} dp_{12} \times
      \int_0^1 p_{21}^{n_{21}} (1-p_{21})^{n_{22}} dp_{21}\\
      &= {n_{1+} \choose n_{12}}{n_{2+} \choose n_{21}}
      \frac{\Gamma(n_{12}+1)\Gamma(n_{11}+1)}{\Gamma(n_{1+} + 2)}
      \cdot \frac{\Gamma(n_{21}+1)\Gamma(n_{22}+1)}{\Gamma(n_{2+} + 2)}\\
      &\approx \Sexpr{exp(log.py.h1)}
    \end{align*}
    The Bayes factor is then
    \begin{align*}
      \mathrm{BF} = \frac{\Pr(\mathbf{n}|H_0)}{\Pr(\mathbf{n}|H_1)}
      \approx \Sexpr{exp(log.py.h0 - log.py.h1)}
    \end{align*}
    Using the Kass and Raftery suggestions for intervals of Bayes Factors, we
    note that $1/\mathrm{BF} \approx \Sexpr{exp(log.py.h1 - log.py.h0)} > 150$,
    so we would say there is strong evidence against the null
    hypothesis of independence.

<<snoqualmie-by-year, cache=T, eval=T, echo=F>>=
p12ByYear <- rep(0, 36)
p21ByYear <- rep(0, 36)
l1.hat <- 0
log.py.year.h1 <- lfactorial(36)
for (i in 1:36) {
  n.tmp <- matrix(0, nrow=2, ncol=2)
  for (j in 2:30) {
    if (z[i,j-1] == 0 && z[i,j] == 0) n.tmp[1,1] <- n.tmp[1,1] + 1
    if (z[i,j-1] == 0 && z[i,j] > 0) n.tmp[1,2] <- n.tmp[1,2] + 1
    if (z[i,j-1] > 0 && z[i,j] == 0) n.tmp[2,1] <- n.tmp[2,1] + 1
    if (z[i,j-1] > 0 && z[i,j] > 0) n.tmp[2,2] <- n.tmp[2,2] + 1
  }
  p12ByYear[i] <- n.tmp[1,2] / sum(n.tmp[1,])
  p21ByYear[i] <- n.tmp[2,1] / sum(n.tmp[2,])
  tpm <- matrix(c(1-p12ByYear[i], p12ByYear[i], p21ByYear[i], 1-p21ByYear[i]),
                nrow=2, byrow=T)
  l1.hat <- l1.hat + sum(n.tmp * log(tpm))
  log.py.year.h1 <- log.py.year.h1 + lchoose(sum(n.tmp[1,]), n.tmp[1,2]) + lchoose(sum(n.tmp[2,]), n.tmp[2,1]) + lgamma(n.tmp[1,2] + 1) + lgamma(n.tmp[1,1] + 1) - lgamma(sum(n.tmp[1,]) + 2) + lgamma(n.tmp[2,1] + 1) + lgamma(n.tmp[2,2] + 1) - lgamma(sum(n.tmp[2,]) + 2)
}
l0.hat <- sum(n * log(p.hat))
lrt.year <- 2 * (l1.hat - l0.hat)
lrt.df <- 2 * 36 - 2
pval.year <- 1 - pchisq(lrt.year, df=lrt.df)
log.py.year.h0 <- log.py.h1
@
  \item Estimating the probabilites $p_{12}$ and $p_{21}$ in each year by the
    MLE, we have below a plot of the logits of the probabilities,
    \begin{center}
<<snoqualmie-logit-plot, eval=T, echo=F, figure=T, fig.height=4, fig.width=6>>=
logit <- function(p) { log(p) - log(1-p) }
plot.df <- data.frame(logit=c(logit(p12ByYear), logit(p21ByYear)),
                      year=rep(1:36, 2),
                      prob=rep(c("p12", "p21"), rep(36, 2)))
ggplot(plot.df, aes(x=year, y=logit, group=prob)) +
  geom_line(aes(linetype=prob, color=prob))
@
    \end{center}
    To test the null hypothesis that the transition probabilities are the same
    every year versus the alternative that they are different every year, we
    need to find $\hat\ell_0$ and $\hat\ell_1$. Letting $n_{ij}^{(k)}$ be the
    number of transitions from state $i$ to state $j$ in year $k$ and
    $p_{ij}^{(k)}$ the transition probabilities for year $k$, we have
    \begin{align*}
      \hat\ell_1
      = \sum_{k=1}^{36} \sum_{i=1}^2 \sum_{j=1}^2 n_{ij}^{(k)} \log \hat{p}_{ij}^{(k)}
      \approx \Sexpr{l1.hat}
    \end{align*}
    whereas for $\hat\ell_0$, we have
    \begin{align*}
      \hat\ell_0
      = \sum_{i=1}^2 \sum_{j=1}^2 n_{ij} \log \hat{p}_{ij}
      \approx \Sexpr{l0.hat}
    \end{align*}
    giving us a likelihood ratio test statistic of
    \begin{align*}
      T = 2(\hat\ell_1 - \hat\ell_0) = \Sexpr{lrt.year}
    \end{align*}
    which follows a $\chi^2_{\Sexpr{lrt.df}}$, where the degrees of freedom arise
    from the fact that under $H_1$ we have $2 \times 36 = 72$ parameters to
    estimate and under $H_0$ we have only $2$ parameters. With $T \sim
    \chi^2_{\Sexpr{lrt.df}}$, we have $\Pr(T > \Sexpr{lrt.year}) \approx
    \Sexpr{pval.year}$, so we would reject the null hypothesis that there
    are common transition probabilities across years (if we assume a cut-off
    of $0.05$).\\
    Under the Bayesian paradigm, we assume $\mathrm{Beta}(1,1)$ priors for
    $p_{12}$, $p_{21}$, and $p_{12}^{(k)}$ and $p_{21}^{(k)}$ for all years
    $k = 1,\ldots,36$. To determine the Bayes factor, we must compute
    $\Pr(\mathbf{n}|H_0)$ and $\Pr(\mathbf{n}|H_0)$. In this case, we have
    already computed $\Pr(\mathbf{n}|H_0) \approx \Sexpr{exp(log.py.year.h0)}$,
    which was the alternative hypothesis in part (c). For
    $\Pr(\mathbf{n}|H_1)$, we have
    \begin{align*}
      \Pr(\mathbf{n}|H_1)
      &= 36! \prod_{k=1}^{36} \int_0^1\int_0^1
      {p_{12}^{(k)}}^{n_{12}^{(k)}} \left(1-p_{12}^{(k)}\right)^{n_{11}^{(k)}}
      {p_{21}^{(k)}}^{n_{21}^{(k)}} \left(1-p_{21}^{(k)}\right)^{n_{22}^{(k)}}
      \left(\frac{\Gamma(2)}{\Gamma(1)\Gamma(1)}\right)^2 dp_{12}^{(k)} dp_{21}^{(k)}
      {n_{1+}^{(k)} \choose n_{12}^{(k)}}{n_{2+}^{(k)} \choose n_{21}^{(k)}}\\
      &= 36! \prod_{k=1}^{36} {n_{1+}^{(k)} \choose n_{12}^{(k)}}
      {n_{2+}^{(k)} \choose n_{21}^{(k)}}
      \frac{\Gamma(n_{12}^{(k)}+1)\Gamma(n_{11}^{(k)}+1)}{\Gamma(n_{1+}^{(k)} + 2)}
      \cdot \frac{\Gamma(n_{21}^{(k)}+1)\Gamma(n_{22}^{(k)}+1)}{\Gamma(n_{2+}^{(k)} + 2)}\\
      &\approx \Sexpr{exp(log.py.year.h1)}
    \end{align*}
    yielding a Bayes factor of
    \begin{align*}
      \mathrm{BF} &= \frac{\Pr(\mathbf{n}|H_0)}{\Pr(\mathbf{n}|H_1)}
      \approx \Sexpr{exp(log.py.year.h0 - log.py.year.h1)}
    \end{align*}
    which, by the suggested intervals of Kass and Raftery, does \textit{not}
    suggest evidence to reject the null hypothesis that transition
    probabilities are common across years.

\end{enumerate}

\subsection*{3}
\begin{enumerate}[(a)]
  \item In the $3 \times 3$ case, with $\mathbf{P} = p_{ij}$ unrestricted, we
    can ``parameterize'' the transition probabilities with
    $\theta = (p_{11}, p_{12}, p_{21}, p_{22}, p_{31}, p_{32})$, yielding
    the transition probability matrix
    \begin{align*}
      \left[\begin{array}{ccc}
          p_{11} & p_{12} & 1 - p_{11} - p_{12} \\
          p_{21} & p_{22} & 1 - p_{21} - p_{22} \\
          p_{31} & p_{32} & 1 - p_{31} - p_{32}
      \end{array} \right]
    \end{align*}
    Using the results on parametric transition probabilities, we use the
    score equations
    \begin{align*}
      \frac{\partial \ell}{\partial \theta_k}
      = \sum_{i,j \in S} \frac{n_{ij}}{p_{ij}(\theta)}
      \frac{\partial p_{ij}(\theta)}{\partial \theta_k} = 0
    \end{align*}
    to find the optimal $\hat\theta$.\\
    With $\theta = (p_{11}, p_{12}, p_{21}, p_{22}, p_{31}, p_{32})$, we have
    for $i \in \{1,2,3\}$,
    \begin{align*}
      \frac{\partial \ell}{\partial p_{i1}}
      &= \frac{n_{i1}}{p_{i1}} - \frac{n_{i3}}{1 - p_{i1} - p_{i2}} = 0\\
      \frac{\partial \ell}{\partial p_{i2}}
      &= \frac{n_{i2}}{p_{i2}} - \frac{n_{i3}}{1 - p_{i1} - p_{i2}} = 0
    \end{align*}
    yielding
    \begin{align*}
      n_{i1} - n_{i1}p_{i1} - n_{i1}p_{i2} = n_{i3}p_{i1}
      &\Rightarrow p_{i1} = \frac{n_{i1} - n_{i1}p_{i2}}{n_{i1} + n_{i3}}\\
      n_{i2} - n_{i2}p_{i1} - n_{i2}p_{i2} &= n_{i3}p_{i2}\\
      n_{i2} - n_{i2}\left(\frac{n_{i1} - n_{i1}p_{i2}}{n_{i1} + n_{i3}}\right) - n_{i2}p_{i2} &= n_{i3}p_{i2}\\
      n_{i1}n_{i2} + n_{i2}n_{i3} - n_{i1}n_{i2} + n_{i1}n_{i2}p_{i2} - n_{i1}n_{i2}p_{i2} - n_{i2}n_{i3}p_{i2} &= n_{i1}n_{i3}p_{i2} + n_{i3}^2p_{i2}\\
      n_{i2}n_{i3} - n_{i2}n_{i3}p_{i2} &= n_{i1}n_{i3}p_{i2} + n_{i3}^2p_{i2}\\
      (n_{i1} + n_{i2} + n_{i3})n_{i3}p_{i2} &= n_{i2}n_{i3}\\
      p_{i2} &= \frac{n_{i2}}{n_{i1}+n_{i2}+n_{i3}}
    \end{align*}
    and thus
    \begin{align*}
      \hat{p}_{i2} &= \frac{n_{i2}}{n_{i1}+n_{i2}+n_{i3}}\\
      \hat{p}_{i1} &= \frac{n_{i1} - n_{i1}\left(\frac{n_{i2}}{n_{i1}+n_{i2}+n_{i3}}\right)}{n_{i1} + n_{i3}}\\
      &= \frac{n_{i1}n_{i1} + n_{i1}n_{i2} + n_{i1}n_{i3} - n_{i1}n_{i2}}{(n_{i1} + n_{i3})(n_{i1} + n_{i2} + n_{i3})}\\
      &= \frac{n_{i1}(n_{i1} + n_{i3})}{(n_{i1} + n_{i3})(n_{i1} + n_{i2} + n_{i3})}\\
      &= \frac{n_{i1}}{n_{i1} + n_{i2} + n_{i3}}
    \end{align*}
    for $i = \{1,2,3\}$, assuming $n_{ij} > 0$ for all $i,j \in S$, which is
    what we expected. That is,
    \begin{align*}
      \hat\theta
      &= (\hat{p}_{11}, \hat{p}_{12}, \hat{p}_{21}, \hat{p}_{22}, \hat{p}_{31}, \hat{p}_{32})\\
      &= \left(
        \frac{n_{11}}{n_{1+}},
        \frac{n_{12}}{n_{1+}}
        \frac{n_{21}}{n_{2+}},
        \frac{n_{22}}{n_{2+}},
        \frac{n_{31}}{n_{3+}},
        \frac{n_{32}}{n_{3+}}
      \right)
    \end{align*}
    We now find the $6 \times 6$ joint asymptotic covariance matrix for
    the MLE $\hat\theta$. We first find the Fisher information, denoting
    $\Info_{(ij,kl)}$ the covariance between $\hat{p}_{ij}$ and $\hat{p}_{kl}$.
    First, note that $\Info_{(ij,kl)} = 0$ for $i \ne k$, leaving a block
    diagonal Fisher information matrix. Thus, we take as example the covariance
    submatrix for $p_{11}$ and $p_{12}$, knowing the rest of the matrix follows
    analogously. We use the result stated in lecture,
    \begin{align*}
      \Info_{km} = \sum_{i,j \in S} \frac{\pi_i(\theta)}{p_{ij}(\theta)} \frac{\partial p_{ij}(\theta)}{\partial \theta_k} \frac{\partial p_{ij}(\theta)}{\partial \theta_k}
    \end{align*}
    We have
    \begin{align*}
      \Info_{(11,11)}
      &= \frac{\pi_1}{p_{11}} + \frac{\pi_1}{1-p_{11} - p_{12}}
      = \frac{\pi_1(1-p_{12})}{p_{11}(1-p_{11} - p_{12})}\\
      \Info_{(11,12)}
      &= \frac{\pi_1}{1-p_{11} - p_{12}}\\
      \Info_{(12,12)}
      &= \frac{\pi_1}{p_{12}} + \frac{\pi_1}{1-p_{11} - p_{12}}
      = \frac{\pi_1(1-p_{11})}{p_{12}(1-p_{11} - p_{12})}
    \end{align*}
    Since the Fisher information is block-diagonal, we can invert the
    $2 \times 2$ submatrix formed by the parameters $p_{11}$ and $p_{12}$.
    Denoting this submatrix $\Info_1$,
    \begin{align*}
      \Info_1^{-1}
      &= \frac{1}{\det(\Info_1)}\left[ \begin{array}{cc}
        \Info_{(12,12)} & \Info_{(11,12)} \\
        \Info_{(11,12)} & \Info_{(11,11)}
      \end{array} \right]
    \end{align*}
    where
    \begin{align*}
      \det(\Info_1)
      &= \Info_{(11,11)} \Info_{(12,12)} - \Info_{(11,12)}^2\\
      &= \frac{\pi_1(1-p_{12})}{p_{11}(1-p_{11} - p_{12})}
      \cdot \frac{\pi_1(1-p_{11})}{p_{12}(1-p_{11} - p_{12})}
      - \frac{\pi_1^2}{(1-p_{11} - p_{12})^2}\\
      &= \frac{\pi_1^2}{(1-p_{11} - p_{12})^2}
      \cdot \left[\frac{(1-p_{12})(1-p_{11})}{p_{11}p_{12}} - 1 \right]\\
      &= \frac{\pi_1^2}{(1-p_{11} - p_{12})^2} \cdot \frac{1-p_{11} - p_{12}}{p_{11}p_{12}}\\
      &= \frac{\pi_1^2}{p_{11}p_{12}(1-p_{11} - p_{12})}
    \end{align*}
    yielding
    \begin{align*}
      \Info_1^{-1}
      &= \frac{1}{\det(\Info_1)}\left[ \begin{array}{cc}
        \Info_{(12,12)} & \Info_{(11,12)} \\
        \Info_{(11,12)} & \Info_{(11,11)}
      \end{array} \right]\\
      &= \frac{p_{11}p_{12}(1-p_{11} - p_{12})}{\pi_1^2}
      \left[ \begin{array}{cc}
        \frac{\pi_1(1-p_{11})}{p_{12}(1-p_{11} - p_{12})} & \frac{\pi_1}{1-p_{11} - p_{12}} \\
        \frac{\pi_1}{1-p_{11} - p_{12}} & \frac{\pi_1(1-p_{12})}{p_{11}(1-p_{11} - p_{12})}
      \end{array} \right]\\
      &= \frac{1}{\pi_1}
      \left[ \begin{array}{cc}
        p_{11}(1-p_{11}) & -p_{11}p_{12} \\
        -p_{11}p_{12} & p_{12}(1-p_{12})
      \end{array} \right]
    \end{align*}
    Thus, the covariance matrix for the MLE is block diagonal, with $3$ blocks,
    where the $i^{th}$ block is given by
    \begin{align*}
      \frac{1}{\pi_i}
      \left[ \begin{array}{cc}
        p_{i1}(1-p_{i1}) & -p_{i1}p_{i2} \\
        -p_{i1}p_{i2} & p_{i2}(1-p_{i2})
      \end{array} \right]
    \end{align*}

  \item For the $9 \times 9$ joint asymptotic covariance matrix for the MLE
    $(\hat{p}_{11}, \hat{p}_{12}, \hat{p}_{13}, \hat{p}_{21}, \hat{p}_{22},
    \hat{p}_{23}, \hat{p}_{31}, \hat{p}_{32}, \hat{p}_{33})$, we again have
    a block diagonal matrix with $3$ blocks. We have the terms
    $\Var(\hat{p}_{i1})$, $\Cov(\hat{p}_{i1}, \hat{p}_{i2})$,
    and $\Var(\hat{p}_{i2})$. Noting that $\hat{p}_{i3} = 1 - \hat{p}_{i1} -
    \hat{p}_{i2}$, we have
    \begin{align*}
      \Var(\hat{p}_{i3})
      &= \Var(1 - \hat{p}_{i1} - \hat{p}_{i2})\\
      &= \Var(\hat{p}_{i1}) + \Var(\hat{p}_{i2}) + 2\Cov(\hat{p}_{i1},\hat{p}_{i2})\\
      &= \frac{1}{\pi_i}\left(p_{i1}(1-p_{i1}) + p_{i2}(1-p_{i2}) - 2p_{i1}p_{i2}\right)\\
      &= \frac{1}{\pi_i}\left(p_{i1}-p_{i1}^2 + p_{i2}-p_{i2}^2 - 2p_{i1}p_{i2}\right)\\
      &= \frac{1}{\pi_i}\left(p_{i1} + p_{i2} - (p_{i1} + p_{i2})^2\right)\\
      &= \frac{1}{\pi_i}\left((1 - p_{i1} - p_{i2})(p_{i1} + p_{i2})\right)\\
      &= \frac{1}{\pi_i}\left(p_{i3}(1-p_{i3})\right)\\
      \Cov(\hat{p}_{i1}, \hat{p}_{i3})
      &= \Cov(\hat{p}_{i1}, 1 - \hat{p}_{i1} - \hat{p}_{i2})\\
      &= -\Var(\hat{p}_{i1}) - \Cov(\hat{p}_{i1}, \hat{p}_{i2})\\
      &= \frac{1}{\pi_i}\left(-p_{i1}(1-p_{i1}) + p_{i1}p_{i2}\right)\\
      &= \frac{1}{\pi_i}\left(-p_{i1}(1-p_{i1} - p_{i2})\right)\\
      &= \frac{1}{\pi_i}\left(-p_{i1} p_{i3}\right)\\
      \Cov(\hat{p}_{i2}, \hat{p}_{i3})
      &= \frac{1}{\pi_i}\left(-p_{i2} p_{i3}\right)
    \end{align*}
    where the last term is arrived at by analogous reasoning to
    $\Cov(\hat{p}_{i1}, \hat{p}_{i3})$.\\
    Thus, we have that the joint asymptotic covariance matrix is block
    diagonal, with $3$ blocks, where the $i^{th}$ block is given by
    \begin{align*}
      \frac{1}{\pi_i}
      \left[ \begin{array}{ccc}
        p_{i1}(1-p_{i1}) & -p_{i1}p_{i2} & -p_{i1}p_{i3} \\
        -p_{i1}p_{i2} & p_{i2}(1-p_{i2}) & -p_{i2}p_{i3} \\
        -p_{i1}p_{i3} & -p_{i2}p_{i3} & p_{i3}(1-p_{i3})
      \end{array} \right]
    \end{align*}

  % TODO
  \item

\end{enumerate}

\end{document}
