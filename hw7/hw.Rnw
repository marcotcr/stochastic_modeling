\documentclass[10pt]{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{listings}
\usepackage{enumerate}
\usepackage{float}
\usepackage{rotating}

\lstset{numbers=left, frame=single, basicstyle=\small\ttfamily}

\usepackage[margin=1in]{geometry}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\indep}{\perp\!\!\!\perp}
\newcommand{\Indicator}[1]{\mathbbm{1}_{\{#1\}}}
\newcommand{\Info}{\mathcal{I}}

\title{\textbf{Stat 516}\\Homework 7}
\author{Alden Timme and Marco Ribeiro}
\date{Due Date: Thursday, December 4}

\begin{document}
<<libaries, eval=T, echo=F, message=F, warning=F>>=
library(ggplot2)
library(xtable)
@

\maketitle
\subsection*{1}
To make notation easier, we rename each sample of the observed data to $Y_i =
(x_i, y_i)$. We also note that:
\begin{align*}
\Sigma = \left[ \begin{array}{cc}
\sigma_1^2 & \rho\sigma_1\sigma_2\\
\rho\sigma_1\sigma_2 & \sigma_2^2 
\end{array}\right]
\end{align*}
The density then becomes:
\begin{align*}
  f_{\sigma_1^2,\sigma_2^2,\rho}(x_i, y_i) =
  \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1 - \rho^2}}\exp\left\{-\frac{1}{2(1 -
  \rho^2)}\left(\frac{x_i^2}{\sigma_1^2} + \frac{y_i^2}{\sigma_2^2} -
  \frac{2\rho x_iy_i}{\sigma_1\sigma_2}\right)\right\}
\end{align*}

\begin{enumerate}[(a)]
\item If there was not missing data,  log likelihood would be
as follows, assuming we had full observed data $X$ s.t. $X_i = (a_i,b_i)$.
\begin{align*}
  \ell(\Sigma) &= C - \frac{n}{2}\log(|\Sigma|) - \frac{1}{2}\sum_{i=1}^{n}{(X_i)^T\Sigma^{-1}X_i}
  \end{align*}
%\begin{align*}
%  L(\sigma_1^2,\sigma_2^2,\rho) &= \prod_{i=1}^{n}{f_{\sigma_1^2,\sigma_2^2,\rho}(x_i, y_i)} \\
%  l(\sigma_1^2,\sigma_2^2,\rho) &= \sum_{i=1}^{n}{-log(2\pi) -log(\sigma_1) -
%  log(\sigma_2) - \frac{1}{2}log(1-\rho^2) - \frac{1}{2(1-\rho^2)}
%  \left(\frac{a_i^2}{\sigma^2_1} + \frac{b_i^2}{\sigma_2^2} - \frac{2\rho
%  a_ib_i}{\sigma_1\sigma_2} \right)}
%\end{align*}
In the E step of EM, we would have to evaluate $\E[\ell(\Sigma)
| Y, \tilde\sigma_1^2, \tilde\sigma_2^2, \tilde\rho]$, where
$(\tilde\sigma_1^2, \tilde\sigma_2^2, \tilde\rho)$ are current guesses of
the parameters, which we abbreviate
to $E[\ell | .]$.
\begin{align*}
  \E[\ell|.] &= c + -\frac{n}{2}\log(|\Sigma|) - \frac{1}{2}\sum_{i=1}^{n}{(\E[X_i|.])^T\Sigma^{-1}\E[X_i|.]}
\end{align*}
We know that in the bivariate normal with zero means,
\begin{align*}
a_i|b_i \sim\ \mathcal{N}\left(\frac{\sigma_1}{\sigma_2}\rho b_i,\, (1-\rho^2)\sigma_1^2\right) \\
b_i|a_i \sim\ \mathcal{N}\left(\frac{\sigma_2}{\sigma_1}\rho a_i,\, (1-\rho^2)\sigma_2^2\right)
\end{align*}
And thus, for $X_i = (a_i,b_i)$ and $Y_i = (x_i, y_i)$:
\begin{align*}
\E\left[a_i|Y, \tilde\sigma_1^2, \tilde\sigma_2^2, \tilde\rho\right] &= \begin{cases}
  x_i  & \text{if } x_i \ne NA \\
  \tilde\rho\tilde\sigma_1y_i/\tilde\sigma_2 & \text{if } x_i  = NA
  \end{cases} \\
\E\left[b_i|Y, \tilde\sigma_1^2, \tilde\sigma_2^2, \tilde\rho\right] &= \begin{cases}
  y_i  & \text{if } y_i \ne NA \\
  \tilde\rho\tilde\sigma_2x_i/\tilde\sigma_1 & \text{if } y_i  = NA
  \end{cases} \\
\E\left[a_i^2|Y, \tilde\sigma_1^2, \tilde\sigma_2^2, \tilde\rho\right] &= \begin{cases}
  x_i^2  & \text{if } x_i \ne NA \\
  \Var(a_i | b_i) + \E[a_i | b_i]^2 = (1 - \tilde\rho^2)\tilde\sigma_1^2 + \frac{\tilde\sigma_1^2}{\tilde\sigma_2^2} \tilde\rho^2 y_i^2 & \text{if } x_i  = NA
  \end{cases} \\
\E\left[b_i^2|Y, \tilde\sigma_1^2, \tilde\sigma_2^2, \tilde\rho\right] &= \begin{cases}
  y_i^2  & \text{if } y_i \ne NA \\
  \Var(b_i | a_i) + \E[b_i | a_i]^2 = (1 - \tilde\rho^2)\tilde\sigma_2^2 + \frac{\tilde\sigma_2^2}{\tilde\sigma_1^2}\tilde\rho^2 x_i^2 & \text{if } y_i  = NA
  \end{cases}
\end{align*}

For the M step, we would optimize $\E[\ell|.]$. We know that when $\mu = 0$ the
MLE for $\Sigma$, which optimizes the log likelihood, is:
\begin{align*}
\hat{\Sigma} = \frac{1}{n}\sum_{i=1}^{n}{\E[X_i X_i^T]}
\end{align*}
And thus:
\begin{align*}
\hat\sigma_1^2 &= \frac{1}{n}\sum_{i=1}^{n}{\E[a_i^2|.]}\\
\hat\sigma_2^2 &= \frac{1}{n}\sum_{i=1}^{n}{\E[b_i^2|.]}\\
\hat\rho &=
\frac{1}{n\hat\sigma_1 \hat\sigma_2}\sum_{i=1}^{n}{\E[a_i|.] \E[b_i|.]}
\end{align*}

<<normal-em, eval=T, echo=F>>=
missdata <- read.table("hw7-missdata-centered.txt", header=T)
x1 <- missdata$X1
x2 <- missdata$X2
n <- nrow(missdata)
BivariateNormalIncompleteLogLikelihood <- function(sigma.1.sq, sigma.2.sq, rho) {
  ll <- 0
  for (i in 1:n) {
    if (is.na(x1[i])) {           # just x2
      ll <- ll - log(2*pi)/2
      ll <- ll - log(sigma.2.sq)/2 - x2[i]^2/(2 * sigma.2.sq)
    } else if (is.na(x2[i])) {    # just x1
      ll <- ll - log(2*pi)/2
      ll <- ll - log(sigma.1.sq)/2 - x1[i]^2/(2 * sigma.1.sq)
    } else {                      # both
      ll <- ll - log(2 * pi)
      ll <- ll - log(sigma.1.sq)/2 - log(sigma.2.sq)/2 - log(1-rho^2)/2
      ll <- ll - (x1[i]^2/sigma.1.sq + x2[i]^2/sigma.2.sq - 2 * rho * x1[i] * x2[i] / sqrt(sigma.1.sq * sigma.2.sq))/(2 * (1-rho^2))
    }
  }
  ll
}
BivariateNormalEM <- function(sigma.1.sq, sigma.2.sq, rho) {
  ll <- BivariateNormalIncompleteLogLikelihood(sigma.1.sq, sigma.2.sq, rho)
  while (T) {
    sigma.hat.1.sq <- 0
    sigma.hat.2.sq <- 0
    sigma.hat.12 <- 0
    for (i in 1:n) {
      if (is.na(x1[i])) {
        ai <- rho * sqrt(sigma.1.sq) * x2[i] / sqrt(sigma.2.sq)
        ai.sq <- sigma.1.sq * (1 - rho^2 + rho^2 * x2[i]^2 / sigma.2.sq)
      } else {
        ai <- x1[i]
        ai.sq <- x1[i]^2
      }
      if (is.na(x2[i])) {
        bi <- rho * sqrt(sigma.2.sq) * x1[i] / sqrt(sigma.1.sq)
        bi.sq <- sigma.2.sq * (1 - rho^2 + rho^2 * x1[i]^2 / sigma.1.sq)
      } else {
        bi <- x2[i]
        bi.sq <- x2[i]^2
      }
      sigma.hat.1.sq <- sigma.hat.1.sq + ai.sq
      sigma.hat.2.sq <- sigma.hat.2.sq + bi.sq
      sigma.hat.12 <- sigma.hat.12 + ai * bi
    }
    sigma.hat.1.sq <- sigma.hat.1.sq / n
    sigma.hat.2.sq <- sigma.hat.2.sq / n
    sigma.hat.12 <- sigma.hat.12 / n
    rho.hat <- sigma.hat.12 / (sqrt(sigma.hat.1.sq * sigma.hat.2.sq))

    sigma.1.sq <- sigma.hat.1.sq
    sigma.2.sq <- sigma.hat.2.sq
    rho <- rho.hat
    new.ll <- BivariateNormalIncompleteLogLikelihood(sigma.1.sq, sigma.2.sq, rho)
    if ((new.ll - ll) < 1e-12) break
    ll <- new.ll
  }
  return(list(sigma.1.sq=sigma.1.sq, sigma.2.sq=sigma.2.sq, rho=rho, loglik=ll))
}
em.results <- BivariateNormalEM(1, 1, 0)
@
\item Using the data from \texttt{hw7-missdata-centered.txt}, and starting with
  $\sigma_1^2 = \sigma_2^2 = 1$ and $\rho=0$, we arrive at the following
  estimates
  \begin{table}[H]
    \centering
    \begin{tabular}{c|c|c}
      $\hat\rho$ & $\hat\sigma_1^2$ & $\hat\sigma_2^2$\\\hline
      \Sexpr{em.results$rho} &
      \Sexpr{em.results$sigma.1.sq} &
      \Sexpr{em.results$sigma.2.sq}
    \end{tabular}
  \end{table}

<<em-random-start, eval=T, echo=F, cache=T>>=
rhos <- c(-0.5, 0, 0.5)
sigma.1.sqs <- c(0.5, 1, 2, 4)
sigma.2.sqs <- c(0.5, 1, 2, 4)
N <- length(rhos) * length(sigma.1.sqs) * length(sigma.2.sqs)
results <- data.frame(rho.start=rep(0, N),
                      sigma.1.start=rep(0, N),
                      sigma.2.start=rep(0, N),
                      rho=rep(0, N),
                      sigma.1.sq=rep(0, N),
                      sigma.2.sq=rep(0, N),
                      loglik=rep(0, N))
idx <- 1
for (rho in rhos) {
  for (sigma.1.sq in sigma.1.sqs) {
    for (sigma.2.sq in sigma.2.sqs) {
      results$rho.start[idx] <- rho
      results$sigma.1.start[idx] <- sigma.1.sq
      results$sigma.2.start[idx] <- sigma.2.sq
      res <- BivariateNormalEM(sigma.1.sq, sigma.2.sq, rho)
      results$sigma.1.sq[idx] <- res$sigma.1.sq
      results$sigma.2.sq[idx] <- res$sigma.2.sq
      results$rho[idx] <- res$rho
      results$loglik[idx] <- res$loglik
      idx <- idx + 1
    }
  }
}
@
\item To see how sensitive the EM algorithm is to starting values for this
  problem, we run it from all combinations of starting values with
  $\rho^{(0)} = -0.5, 0, 0.5$, ${\sigma_1^2}^{(0)} = 0.5, 1, 2, 4$, and
  ${\sigma_2^2}^{(0)} = 0.5, 1, 2, 4$. For all $3 \times 4 \times 4 = 48$
  starting values, we arrive at the same results as in part (b). While the EM
  algorithm is known to often be sensitive to starting value, for this problem
  it appears to be robust.
%<<em-multistart-table, eval=T, echo=F, results='asis'>>=
%colnames(results) <- c("$\\rho^{(0)}$", "${\\sigma_1^2}^{(0)}$",
%                       "${\\sigma_2^2}^{(0)}$", "$\\hat\\rho$",
%                       "$\\hat\\sigma_1^2$", "$\\hat\\sigma_2^2$",
%                       "$\\hat\\ell(x)$")
%print(xtable(results, caption="EM from multiple starting values"),
%      sanitize.colnames.function = function(x){x}, include.rownames=F)
%@

\end{enumerate}


\subsection*{2}
\begin{enumerate}[(a)]
<<forward-backward-naive, eval=T, echo=F, cache=T>>=
ForwardBackwardNaive <- function(x, emissions, transitions, initials) {
  numStates <- nrow(emissions)
  n <- length(x)
  backward <- matrix(0, nrow=n, ncol=numStates)
  backward[n,] <- 1
  for (t in (n-1):1) {
    for (i in 1:numStates) {
      backward[t,i] <- sum(transitions[i,] * emissions[,x[t+1]] * backward[t+1,])
    }
  }
  forward <- matrix(0, nrow=n, ncol=numStates)
  forward[1,] <- emissions[,x[1]] * initials
  for (t in 1:(n-1)) {
    for (i in 1:numStates) {
      forward[t+1,i] <- emissions[i,x[t+1]] * sum(forward[t,] * transitions[,i])
    }
  }
  list(backward=backward, forward=forward)
}
SimulateSequence <- function(P, E, nu, n) {
  numStates <- nrow(P)
  numEmitStates <- ncol(E)
  z <- rep(0, n)
  x <- rep(0, n)
  z[1] <- sample(numStates, 1, prob=nu)
  x[1] <- sample(numEmitStates, 1, prob=E[z[1],])
  for (i in 1:(n-1)) {
    z[i+1] <- sample(numStates, 1, prob=P[z[i],])
    x[i+1] <- sample(numEmitStates, 1, prob=E[z[i+1],])
  }
  list(x=x, z=z)
}
P <- matrix(c(0.95, 0.05, 0.10, 0.90), byrow=T, nrow=2)
E <- matrix(c(rep(1/6,6), rep(0.1,5), 0.5), byrow=T, nrow=2)
nu <- c(0.5, 0.5)
GetBreakdown <- function(seed) {
  set.seed(seed)
  res <- SimulateSequence(P, E, nu, 1000)
  for (len in 300:length(res$x)) {
    x <- res$x[1:len]
    fb <- ForwardBackwardNaive(x, E, P, nu)
    if (any(fb$backward[1,] == 0) || any(fb$forward[len,] == 0)) {
      return(len)
    }
  }
  return(1000)
}
breakdowns <- sapply(1:10, GetBreakdown)
@
\item Using a naive implementation of the forward-backward equations, we run
  into numerical issues at sequence lengths of around $420$ to $440$.


<<forward-backward-log, eval=T, echo=F, cache=T>>=
LogSum <- function(logA, logB) {
  if (logB > logA) {
    tmp <- logA
    logA <- logB
    logB <- tmp
  }
  logA + log(1 + exp(logB - logA))
}
ForwardBackwardLog <- function(x, emissions, transitions, initials) {
  numStates <- nrow(emissions)
  n <- length(x)
  logBackward <- matrix(0, nrow=n, ncol=numStates)
  logBackward[n,] <- 0
  for (t in (n-1):1) {
    for (i in 1:numStates) {
      logBackward[t,i] <- log(transitions[i,1]) + log(emissions[1,x[t+1]]) + logBackward[t+1,1]
      for (j in 2:numStates) {
        logB <- log(transitions[i,j]) + log(emissions[j,x[t+1]]) + logBackward[t+1,j]
        logBackward[t,i] <- LogSum(logBackward[t,i], logB)
      }
    }
  }
  logForward <- matrix(0, nrow=n, ncol=numStates)
  logForward[1,] <- log(emissions[,x[1]]) + log(initials)
  for (t in 1:(n-1)) {
    for (i in 1:numStates) {
      logForward[t+1,i] <- logForward[t,1] + log(transitions[1,i])
      for (j in 2:numStates) {
        logF <- logForward[t,j] + log(transitions[j,i])
        logForward[t+1,i] <- LogSum(logForward[t+1,i], logF)
      }
      logForward[t+1,i] <- logForward[t+1,i] + log(emissions[i,x[t+1]])
    }
  }
  list(logBackward=logBackward, logForward=logForward)
}
@
\item Yes, when keeping everything on the log-scale and using the $\log(a+b)$
  formula given, we are able to succesfully compute the ($\log$) forward and
  backward probabilities without running into numerical issues.

<<forward-backward-scaled, eval=T, echo=F>>=
ForwardBackwardRescaled <- function(Y, P, E, nu) {
  n = length(Y)
  num_states <- nrow(P)
  b = matrix(1, n, num_states)
  a = matrix(0, n, num_states)
  c = rep(0, n)
  for (i in 1:num_states) {
    a[1,i] = nu[i] * E[i, Y[1]]
  }
  c[1] = sum(a[1,])
  a[1,] = a[1,] / c[1]
  for (t in 1:(n-1)) {
    for (i in 1:num_states) {
      a[t+1,i] = E[i, Y[t+1]] * sum(a[t,] * P[,i])
    }
    c[t+1] = sum(a[t+1,])
    a[t+1,] = a[t+1,] / c[t+1]
  }
  for (t in (n-1):1 ) {
    for (i in 1:num_states ) {
      b[t,i] = sum(P[i,] * E[, Y[t+1]] * b[t+1,]) / c[t+1]
    }
  }
  list(a=a, b=b, c=c)
}
@
<<forward-backward-probs, eval=T, echo=F>>=
# probability that t-th state takes on value i
GetProbabilityLog <- function(forwardBackwardLog, t, i) {
  logNumerator <- forwardBackwardLog$logForward[t,i] + forwardBackwardLog$logBackward[t,i]
  numStates <- ncol(forwardBackwardLog$logForward)
  logDenominator <- forwardBackwardLog$logForward[t,1] + forwardBackwardLog$logBackward[t,1]
  for (j in 2:numStates) {
    logDenominator <- LogSum(logDenominator, forwardBackwardLog$logForward[t,j] + forwardBackwardLog$logBackward[t,j])
  }
  return(exp(logNumerator - logDenominator))
}
# probability that t-th state takes on value i
GetProbabilityNaive <- function(forwardBackwardNaive, t, i) {
  numerator <- forwardBackwardNaive$forward[t,i] * forwardBackwardNaive$backward[t,i]
  return(numerator/sum(forwardBackwardNaive$forward[t,] * forwardBackwardNaive$backward[t,]))
}
# probability that t-th state takes on value i
GetProbabilityRescaled <- function(forwardBackwardRescaled, t, i) {
  forwardBackwardRescaled$a[t,i] * forwardBackwardRescaled$b[t,i]
}
#res <- SimulateSequence(P, E, nu, 10000)
#fbLog <- ForwardBackwardLog(res$x, E, P, nu)
#fbRescaled <- ForwardBackwardRescaled(res$x, E, P, nu)
#fbNaive <- ForwardBackwardNaive(res$x, E, P, nu)
#GetProbabilityLog(fbLog, 5000, 1)
#GetProbabilityNaive(fbNaive, 5000, 1)
#GetProbabilityRescaled(fbRescaled, 5000, 1)
@
\item Yes, modifying (a) to work with rescaled probabilities, we have no
  problem with longer sequences as we did with the naive implementation from
  (a). We get the same answers with the rescaled probabilities as we do with
  part (b), where we compute all probabilities on the log-scale.

\end{enumerate}

\subsection*{3}
We analyzed the data using a hidden Markov model, with 2 hidden states,
and conditionally upon the state a multinomial observation with 16 categories.
We implemented the Baum-Welch algorithm with appropriately rescaled backward and
forward probabilities, and used as stopping criteria a difference of log
likelihoods smaller then $\epsilon = 0.01$ between iterations. We ran the
algorithm with random initial parameters 5 times, taking the best final
estimates (higher final log likelihood). Figure 1 shows the log likelihood over
EM iterations for the best run.
\begin{figure}[H]
\centering
\includegraphics[scale=.6]{ll.png}
\caption{Log likelihood per iteration of the Baum-Welch algorithm.}
\end{figure}

Figure 2 is a visualization of the assigned emission probabilities
to each wind direction, for each hidden state. It is clear from the figure that
two different wind patterns are picked up: one that favors northwest winds,
and one that favors southeast winds. The transition matrix we find with the
Baum-Welch algorithm is
\begin{align*}
  \hat{P}
  &= \left[\begin{array}{cc}
    0.9631 & 0.0369 \\
    0.0313 & 0.9687
  \end{array}\right]
\end{align*}
so the latent state is far more likely to self-transition than to switch. This
makes some intuitive sense given that state 1 shows more north-westernly winds
while state 2 shows more south-easternly winds, and we would not expect
frequent changes between the two.

\begin{figure}[H]
\centering
\includegraphics[scale=.7]{grid.png}
\caption{Emission probabilities for the two hidden states.}
\end{figure}

We used the Viterbi algorithm to estimate the most likely sequence of underlying
states over the four years of data. However, due to fairly frequent changes
between latent states, a raw plot of the latent states over the four years is
not very helpful. Instead, we show a smoothed version of the latent states. We
use a sliding window of one month and compute a weighted average within that
window, with weights proportional to the density of a normal distribution with
standard deviation one quarter of the window size (ie like a discrete
convolution). In Figure 3, we show the
resulting smoothed latent states, with the months along the x-axis.
We can see seasonal pattern, where the hidden state 1 is more
pronounced between May and August, and hidden state 2 is more pronounced between
September and March. According to Capetown
Magazine\footnote{http://www.capetownmagazine.com/weather-cape-town/cape-town-four-seasons-in-one-day/160\_22\_17571},
there are two noteworthy wind patterns in Capetown (close to where this data was
collected): (1) a southeaster wind nicknamed Cape
Doctor\footnote{http://www.capetownmagazine.com/weather/Cape-Towns-Cape-Doctor/160\_22\_17572},
which blows from August to April, and (2) northwestern winds that blow from May
to August. While we don't know how trustworthy this information is (we are no
experts in wind, South Africa, or winds in South Africa), this seems to coincide
nicely with our results, as this is exactly the pattern we found in our
analysis.

\begin{figure}[H]
\centering
\includegraphics[scale=.7]{mov_average.png}
\caption{Moving average of the hidden states.}
\end{figure}


\newpage
\appendix
\subsection*{Code}
\subsubsection*{1(b) EM for bivariate Normal missing data}
<<normal-em, eval=F, echo=T>>=
@
\subsubsection*{1(c) Many random starting values}
<<em-random-start, eval=F, echo=T>>=
@
\subsubsection*{2(a) Naive forward-backward}
<<forward-backward-naive, eval=F, echo=T>>=
@
\subsubsection*{2(b) Log-scale forward-backward}
<<forward-backward-log, eval=F, echo=T>>=
@
\subsubsection*{2(c) Scaled forward-backward}
<<forward-backward-scaled, eval=F, echo=T>>=
@
\subsubsection*{3}
We implemented the Baum-Welch and Viterbi algorithms, in addition to using the
(rescaled) Forward-Backward algorithm we implemented in problem 2(c). Code for
the relevant functions is replicated below. For any other code (e.g. for plots)
let us know.
<<baum-welch-etc, eval=F, echo=T>>=
# Probability of state t (vector). fb is the return of the function above
Gamma <- function(fb, t) {
  return(fb$a[t,] * fb$b[t,])
}
# Probability of state t-1 = i, t=j. fb is the return of the function above
Csi <- function(fb, t, i, j, E, P) {
  1 / fb$c[t] * fb$a[t - 1, i] * E[j, Y[t]] * P[i,j] * fb$b[t,j]
}
BaumWelch <- function(Y, theta0) {
  n = length(Y)
  nu = theta0$nu
  P = theta0$P
  E = theta0$E
  num_states <- nrow(P)
  previous_logpy = 0
  while (TRUE) {
    z = ForwardBackwardRescaled(Y, P, E, nu)
    logpy = sum(log(z$c))
    nu = Gamma(z, 1)
    new_P = P
    # Computing P
    for (i in 1:num_states) {
      for (j in 1:num_states) {
        new_P[i,j] = sum(Csi(z, 2:n, i, j, E, P))
      }
      new_P[i,] = new_P[i,] / sum(new_P[i,])
    }
    P = new_P
    # Computing E
    norm = apply(Gamma(z, 1:n),2, sum)
    for (i in 1:16) {
      E[,i] = apply((Y[1:n] == i) * Gamma(z, 1:n), 2, sum) / norm
    }
    if (abs(logpy - previous_logpy) < .01) {
      break;
    }
    previous_logpy = logpy
    print(logpy)
  }
  return (list(nu=nu,P=P,E=E))
}
Viterbi <- function(Y, P, E, nu) {
  n = length(Y)
  num_states <- nrow(P)
  d = matrix(0, n, num_states)
  f = matrix(0, n, num_states)
  d[1,] = log(nu) + log(E[,Y[1]])
  for (t in 2:n) {
    d[t,] = apply(d[t-1,] + log(P), 2, max) + log(E[, Y[t]])
    f[t,] = apply(d[t-1,] + log(P), 2, which.max)
  }
  y = rep(0, n)
  y[n] = which.max(d[n,])
  for (t in (n-1):1) {
    y[t] = f[t+1,y[t+1]]
  }
  return (y)
}
@

\end{document}
